{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install underthesea\n",
    "!pip install evaluate \n",
    "!pip install rouge_score\n",
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import AutoTokenizer, MBartForConditionalGeneration, AutoConfig, TrainingArguments, Trainer\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "import torch.nn as nn\n",
    "from copy import deepcopy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize APP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_path = \"vinai/bartpho-word\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SoftPrompt(nn.Module):\n",
    "    def __init__(self, encoder, plm_embed: nn.Embedding, n_prompt: int = 1000,\n",
    "                 embed_size:int=1024):\n",
    "        super().__init__()\n",
    "        self.plm_embed = plm_embed\n",
    "        #self.encoder = encoder\n",
    "        self.n_prompt = n_prompt\n",
    "        self.list_prompts = [nn.parameter.Parameter(torch.randn(embed_size, dtype=torch.float)) for i in range(n_prompt)]\n",
    "        #for prompt in self.list_prompts:\n",
    "        #    nn.init.kaiming_uniform_(prompt)\n",
    "        self.list_prompts = nn.ParameterList(self.list_prompts)\n",
    "        self.attent = nn.MultiheadAttention(embed_dim=embed_size, num_heads=32, batch_first=True)\n",
    "        \n",
    "    def inject(self, tokens):\n",
    "        attention_mask = (tokens != 1).float()\n",
    "        ori = self.plm_embed(tokens)\n",
    "        features = ori #self.encoder(tokens, attention_mask=attention_mask)[0]\n",
    "        list_prompts = torch.cat([i.unsqueeze(0) for i in self.list_prompts]).unsqueeze(0).repeat(tokens.size(0), 1, 1)\n",
    "        features, _ = self.attent(features, list_prompts, list_prompts)\n",
    "        return features + ori\n",
    "        \n",
    "    def forward(self, tokens):\n",
    "        return self.inject(tokens)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0766,  0.0526, -0.0167,  ..., -0.0088, -0.0227, -0.0014],\n",
       "         [ 0.0542,  0.0680,  0.0145,  ..., -0.0073, -0.0226,  0.0119],\n",
       "         [ 0.0629,  0.0345,  0.0362,  ..., -0.0631, -0.0319, -0.0133]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soft = SoftPrompt(deepcopy(model.get_encoder()), model.get_encoder().embed_tokens).to(\"cuda\")\n",
    "soft(torch.tensor([[4,5,6]], device=\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.set_input_embeddings(SoftPrompt(deepcopy(model.get_encoder()), model.get_encoder().embed_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/jupyter/.cache/huggingface/datasets/OpenHust___csv/OpenHust--vietnamese-summarization-0917cc8d0d28c72d/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e5715b0651c4682b2ef160fcb833b0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(path=\"OpenHust/vietnamese-summarization\", data_files=\"herding_bio_medicine.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "small= dataset[\"train\"]\n",
    "\n",
    "train, test = small.train_test_split(train_size=0.8, seed=0).values()\n",
    "train, dev = train.train_test_split(test_size=0.125, seed=0).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cho bác sĩ biết bạn đang nghi ngờ bản thân bị viêm dạ dày và yêu cầu tập trung kiểm tra vùng bụng. Mang theo danh sách ghi rõ triệu chứng bạn đang gặp phải cho bác sĩ xem. Bác sĩ sẽ tìm ra “triệu chứng đáng báo động” cho thấy bạn cần được chăm sóc khẩn cấp. Triệu chứng báo động mà bạn cần cho bác sĩ biết gồm có:  Nôn ra máu hoặc mật Phân có màu đen như hắc ín (đại tiện máu đen) Chán ăn, biếng ăn hoặc sụt cân (nhiều hơn 3 kg) Thiếu máu (dấu hiệu da tái, mệt mỏi, ốm yếu hoặc chóng mặt) Cảm giác chướng bụng Cho bác sĩ biết nếu bạn trên 55 tuổi. Mẫu máu sẽ được bác sĩ đưa đến phòng thí nghiệm để phân tích. Tại phòng thí nghiệm, chuyên viên sẽ tiến hành các xét nghiệm sau:  Xét nghiệm máu toàn bộ (CBC) để kiểm tra bệnh thiếu máu Xét nghiệm Amylase và Lipase để sàng lọc bệnh tuyến tụy Xét nghiệm chức năng gan và chức năng thận để đánh giá tình trạng mất nước và các nguyên nhân khác gây ra triệu chứng nếu bạn nôn mửa Xét nghiệm Guaiac phân để tìm máu ẩn (không nhìn thấy trong phân)  Xét nghiệm Urea trong hơi thở hoặc trong phân/máu để tìm ra vi khuẩn Helicobacter Pylori Nếu nhận thấy triệu chứng đáng lo, bác sĩ có thể tiến hành nội soi cho bạn. Bác sĩ sẽ đưa một máy quay nhỏ được gắn vào ống dài, linh hoạt xuống cổ họng. Máy quay sẽ chạm đủ sâu để giúp quan sát thực quản, dạ dày và một phần ruột non. Nếu xét nghiệm cho kết quả âm tính với khuẩn H. Pylori nhưng vẫn có triệu chứng, bạn có thể chọn phương pháp nội soi tự chọn.  Bạn có thể yêu cầu bác sĩ cho dùng thuốc an thần khi nội soi để thư giãn hơn. Mặc dù cảm thấy hơi áp lực nhưng bạn sẽ không thấy đau khi nội soi. Bác sĩ sẽ quan sát để tìm ra những vết loét, mòn, khối u và những bất thường khác. Bác sĩ cũng có thể lấy sinh thiết để xét nghiệm trong phòng thí nghiệm.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Document\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Unnamed: 0', 'Document', 'Summary', 'Dataset'],\n",
       "    num_rows: 1066\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from underthesea import pos_tag\n",
    "\n",
    "def add_prompt(text):\n",
    "    words = pos_tag(text)\n",
    "    txt = \"\"\n",
    "    for word in words:\n",
    "        if word[1] in [\"N\", \"V\", \"A\"]:\n",
    "            txt += word[0] + \" {} \".format(word[1])\n",
    "       \n",
    "        else:\n",
    "            txt += word[0] + \" \"\n",
    "    return txt\n",
    "\n",
    "def split_source_target(db):\n",
    "    docs = [add_prompt(i) for i in db[\"Document\"]]\n",
    "    sums = [i for i in db[\"Summary\"]]\n",
    "    return docs, sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_dataset(db, max_length, batch_size):\n",
    "    tokenized = {\"input_ids\":[], \"attention_mask\":[]}\n",
    "    for i in tqdm(range(0, len(db), batch_size)):\n",
    "        encoded = tokenizer(db[i:i+batch_size], max_length=max_length, padding=\"max_length\", truncation=True)\n",
    "        tokenized[\"input_ids\"] += encoded[\"input_ids\"]\n",
    "        tokenized[\"attention_mask\"] += encoded[\"attention_mask\"]\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/jupyter/.cache/huggingface/datasets/OpenHust___csv/OpenHust--vietnamese-summarization-0917cc8d0d28c72d/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-c94bd23634033845.arrow\n",
      "Loading cached processed dataset at /home/jupyter/.cache/huggingface/datasets/OpenHust___csv/OpenHust--vietnamese-summarization-0917cc8d0d28c72d/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-fd8133ca44802265.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_sample_data(data):\n",
    "    # Max token size is 14536 and 215 for inputs and labels, respectively.\n",
    "    # Here I restrict these token size.\n",
    "    docs = data[\"Document\"]\n",
    "    sums = data[\"Summary\"]\n",
    "    input_feature = tokenizer(docs, truncation=True, max_length=1024)\n",
    "    label = tokenizer(sums, truncation=True, max_length=1024)\n",
    "    return {\n",
    "    \"input_ids\": input_feature[\"input_ids\"],\n",
    "    \"attention_mask\": input_feature[\"attention_mask\"],\n",
    "    \"labels\": label[\"input_ids\"],\n",
    "    }\n",
    "\n",
    "train = train.map(\n",
    "  tokenize_sample_data,\n",
    "  remove_columns=[\"Summary\", \"Document\", \"Dataset\"],\n",
    "  batched=True,\n",
    "  batch_size=128)\n",
    "\n",
    "test = test.map(\n",
    "  tokenize_sample_data,\n",
    "  remove_columns=[\"Summary\", \"Document\", \"Dataset\"],\n",
    "  batched=True,\n",
    "  batch_size=128)\n",
    "\n",
    "dev = dev.map(\n",
    "  tokenize_sample_data,\n",
    "  remove_columns=[\"Summary\", \"Document\", \"Dataset\"],\n",
    "  batched=True,\n",
    "  batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "  tokenizer,\n",
    "  model=model,\n",
    "  return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = evaluate.load(\"rouge\")\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    #predictions = predictions[:, :-1]\n",
    "    #import pdb\n",
    "    #pdb.set_trace()\n",
    "    predictions[predictions == -100] = 1\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    #labels = labels[labels !=-100]\n",
    "    labels = np.where(labels != -100, labels, tokenizer.eos_token_id)\n",
    "    \n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True,)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    return {k: round(v, 4) for k, v in result.items()}\n",
    "\n",
    "def compute_bleu(eval_preds):\n",
    "    \n",
    "    y_true = eval_preds.label_ids\n",
    "    y_pred = eval_preds.predictions\n",
    "    y_true = np.where(y_true != -100, y_true, tokenizer.pad_token_id)\n",
    "    y_pred = np.where(y_pred != -100, y_true, tokenizer.pad_token_id)\n",
    "    #import pdb\n",
    "    #pdb.set_trace()\n",
    "    metric = load_metric('bleu')\n",
    "    #import pdb\n",
    "    #pdb.set_trace()\n",
    "    try:\n",
    "        y_true = tokenizer.batch_decode(y_true, skip_special_tokens=True)\n",
    "        y_pred = tokenizer.batch_decode(y_pred, skip_special_tokens=True)\n",
    "        y_true = [[i.split()] for i in y_true]\n",
    "        y_pred= [i.split() for i in y_pred]\n",
    "        report = metric.compute(predictions=y_pred, references=y_true)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        import pdb\n",
    "        pdb.set_trace()\n",
    "    bleu = report['bleu'] * 100\n",
    "    print(bleu)\n",
    "    return {\"bleu\":bleu}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "args = Seq2SeqTrainingArguments(output_dir=\"OpenHust/open-bart-herding-1024-no-prompt\",\n",
    "                                evaluation_strategy=\"epoch\",\n",
    "                                save_strategy=\"epoch\",\n",
    "                               per_device_train_batch_size=2,\n",
    "                               per_device_eval_batch_size=2,\n",
    "                               learning_rate=1e-4,\n",
    "                               weight_decay=1e-2,\n",
    "                               load_best_model_at_end =True,\n",
    "                               predict_with_generate=True,\n",
    "                               num_train_epochs=5,\n",
    "                               logging_strategy=\"epoch\",\n",
    "                               generation_max_length=1024,\n",
    "                                save_total_limit = 1,\n",
    "                               fp16=True,)\n",
    "\n",
    "trainer = Seq2SeqTrainer(model=model, \n",
    "                        args=args,\n",
    "                         data_collator=data_collator,\n",
    "                        train_dataset=train,\n",
    "                        eval_dataset=dev,\n",
    "                        tokenizer=tokenizer,\n",
    "                        compute_metrics=compute_metrics,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Unnamed: 0', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 2131\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: Unnamed: 0. If Unnamed: 0 are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 7455\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 18640\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1467: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18640' max='18640' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18640/18640 1:50:04, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.212100</td>\n",
       "      <td>2.765098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.117000</td>\n",
       "      <td>2.411663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.286900</td>\n",
       "      <td>2.351274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.664000</td>\n",
       "      <td>2.427753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.294200</td>\n",
       "      <td>2.513626</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: Unnamed: 0. If Unnamed: 0 are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1066\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to kaggle/working/other/OpenHust/open-bart-herding-1024-no-prompt/checkpoint-3728\n",
      "Configuration saved in kaggle/working/other/OpenHust/open-bart-herding-1024-no-prompt/checkpoint-3728/config.json\n",
      "Model weights saved in kaggle/working/other/OpenHust/open-bart-herding-1024-no-prompt/checkpoint-3728/pytorch_model.bin\n",
      "tokenizer config file saved in kaggle/working/other/OpenHust/open-bart-herding-1024-no-prompt/checkpoint-3728/tokenizer_config.json\n",
      "Special tokens file saved in kaggle/working/other/OpenHust/open-bart-herding-1024-no-prompt/checkpoint-3728/special_tokens_map.json\n",
      "added tokens file saved in kaggle/working/other/OpenHust/open-bart-herding-1024-no-prompt/checkpoint-3728/added_tokens.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1467: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: Unnamed: 0. If Unnamed: 0 are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1066\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to kaggle/working/other/OpenHust/open-bart-herding-1024-no-prompt/checkpoint-7456\n",
      "Configuration saved in kaggle/working/other/OpenHust/open-bart-herding-1024-no-prompt/checkpoint-7456/config.json\n",
      "Model weights saved in kaggle/working/other/OpenHust/open-bart-herding-1024-no-prompt/checkpoint-7456/pytorch_model.bin\n",
      "tokenizer config file saved in kaggle/working/other/OpenHust/open-bart-herding-1024-no-prompt/checkpoint-7456/tokenizer_config.json\n",
      "Special tokens file saved in kaggle/working/other/OpenHust/open-bart-herding-1024-no-prompt/checkpoint-7456/special_tokens_map.json\n",
      "added tokens file saved in kaggle/working/other/OpenHust/open-bart-herding-1024-no-prompt/checkpoint-7456/added_tokens.json\n",
      "Deleting older checkpoint [kaggle/working/other/OpenHust/open-bart-herding-1024-no-prompt/checkpoint-3728] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: Unnamed: 0. If Unnamed: 0 are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1066\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to kaggle/working/other/OpenHust/open-bart-herding-1024-no-prompt/checkpoint-11184\n",
      "Configuration saved in kaggle/working/other/OpenHust/open-bart-herding-1024-no-prompt/checkpoint-11184/config.json\n",
      "Model weights saved in kaggle/working/other/OpenHust/open-bart-herding-1024-no-prompt/checkpoint-11184/pytorch_model.bin\n",
      "tokenizer config file saved in kaggle/working/other/OpenHust/open-bart-herding-1024-no-prompt/checkpoint-11184/tokenizer_config.json\n",
      "Special tokens file saved in kaggle/working/other/OpenHust/open-bart-herding-1024-no-prompt/checkpoint-11184/special_tokens_map.json\n",
      "added tokens file saved in kaggle/working/other/OpenHust/open-bart-herding-1024-no-prompt/checkpoint-11184/added_tokens.json\n",
      "Deleting older checkpoint [kaggle/working/other/OpenHust/open-bart-herding-1024-no-prompt/checkpoint-7456] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1467: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: Unnamed: 0. If Unnamed: 0 are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1066\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to kaggle/working/other/OpenHust/open-bart-herding-1024-no-prompt/checkpoint-14912\n",
      "Configuration saved in kaggle/working/other/OpenHust/open-bart-herding-1024-no-prompt/checkpoint-14912/config.json\n",
      "Model weights saved in kaggle/working/other/OpenHust/open-bart-herding-1024-no-prompt/checkpoint-14912/pytorch_model.bin\n",
      "tokenizer config file saved in kaggle/working/other/OpenHust/open-bart-herding-1024-no-prompt/checkpoint-14912/tokenizer_config.json\n",
      "Special tokens file saved in kaggle/working/other/OpenHust/open-bart-herding-1024-no-prompt/checkpoint-14912/special_tokens_map.json\n",
      "added tokens file saved in kaggle/working/other/OpenHust/open-bart-herding-1024-no-prompt/checkpoint-14912/added_tokens.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: Unnamed: 0. If Unnamed: 0 are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1066\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to kaggle/working/other/OpenHust/open-bart-herding-1024-no-prompt/checkpoint-18640\n",
      "Configuration saved in kaggle/working/other/OpenHust/open-bart-herding-1024-no-prompt/checkpoint-18640/config.json\n",
      "Model weights saved in kaggle/working/other/OpenHust/open-bart-herding-1024-no-prompt/checkpoint-18640/pytorch_model.bin\n",
      "tokenizer config file saved in kaggle/working/other/OpenHust/open-bart-herding-1024-no-prompt/checkpoint-18640/tokenizer_config.json\n",
      "Special tokens file saved in kaggle/working/other/OpenHust/open-bart-herding-1024-no-prompt/checkpoint-18640/special_tokens_map.json\n",
      "added tokens file saved in kaggle/working/other/OpenHust/open-bart-herding-1024-no-prompt/checkpoint-18640/added_tokens.json\n",
      "Deleting older checkpoint [kaggle/working/other/OpenHust/open-bart-herding-1024-no-prompt/checkpoint-14912] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from kaggle/working/other/OpenHust/open-bart-herding-1024-no-prompt/checkpoint-11184 (score: 2.351273536682129).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=18640, training_loss=1.514847581069357, metrics={'train_runtime': 6605.0956, 'train_samples_per_second': 5.643, 'train_steps_per_second': 2.822, 'total_flos': 5.402978719572787e+16, 'train_loss': 1.514847581069357, 'epoch': 5.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: Unnamed: 0. If Unnamed: 0 are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2131\n",
      "  Batch size = 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1066' max='1066' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1066/1066 22:34]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.252601146697998,\n",
       " 'eval_rouge1': 0.5816,\n",
       " 'eval_rouge2': 0.3122,\n",
       " 'eval_rougeL': 0.4205,\n",
       " 'eval_rougeLsum': 0.4204,\n",
       " 'eval_gen_len': 50.3585,\n",
       " 'eval_runtime': 1414.994,\n",
       " 'eval_samples_per_second': 1.506,\n",
       " 'eval_steps_per_second': 0.753}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset=test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-30T04:37:46.432349Z",
     "iopub.status.busy": "2023-05-30T04:37:46.431978Z",
     "iopub.status.idle": "2023-05-30T04:38:03.643358Z",
     "shell.execute_reply": "2023-05-30T04:38:03.640883Z",
     "shell.execute_reply.started": "2023-05-30T04:37:46.432313Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer.evaluate(eval_dataset=dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-30T04:30:27.985924Z",
     "iopub.status.busy": "2023-05-30T04:30:27.985533Z",
     "iopub.status.idle": "2023-05-30T04:30:27.991865Z",
     "shell.execute_reply": "2023-05-30T04:30:27.990848Z",
     "shell.execute_reply.started": "2023-05-30T04:30:27.985893Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate(inputs, num_returns=1):\n",
    "    inputs = tokenizer.encode(inputs, return_tensors=\"pt\", max_length = 1024, padding = True, truncation = True).to(device)\n",
    "    # outputs = model.generate(inputs, max_length = 1024, num_beams = 10, )\n",
    "    outputs = model.generate(inputs, generation_config=genConfig)\n",
    "    #outputs = model.generate(inputs, max_length = 1024, num_beams = 5,\n",
    "    #                        num_beam_groups = 5, num_return_sequences = num_returns, no_repeat_ngram_size = 3)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-30T04:30:29.783432Z",
     "iopub.status.busy": "2023-05-30T04:30:29.782737Z",
     "iopub.status.idle": "2023-05-30T04:30:29.930789Z",
     "shell.execute_reply": "2023-05-30T04:30:29.929822Z",
     "shell.execute_reply.started": "2023-05-30T04:30:29.783400Z"
    }
   },
   "outputs": [],
   "source": [
    "doc = train[\"Document\"][100]\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-30T04:30:31.276265Z",
     "iopub.status.busy": "2023-05-30T04:30:31.275887Z",
     "iopub.status.idle": "2023-05-30T04:30:59.468314Z",
     "shell.execute_reply": "2023-05-30T04:30:59.467215Z",
     "shell.execute_reply.started": "2023-05-30T04:30:31.276234Z"
    }
   },
   "outputs": [],
   "source": [
    "generate(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-30T04:11:50.672559Z",
     "iopub.status.busy": "2023-05-30T04:11:50.672057Z",
     "iopub.status.idle": "2023-05-30T04:11:50.805226Z",
     "shell.execute_reply": "2023-05-30T04:11:50.804228Z",
     "shell.execute_reply.started": "2023-05-30T04:11:50.672516Z"
    }
   },
   "outputs": [],
   "source": [
    "train[\"Summary\"][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-28T18:29:49.004638Z",
     "iopub.status.idle": "2023-05-28T18:29:49.005546Z",
     "shell.execute_reply": "2023-05-28T18:29:49.005284Z",
     "shell.execute_reply.started": "2023-05-28T18:29:49.005258Z"
    }
   },
   "outputs": [],
   "source": [
    "help(trainer.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu110.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m108"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
